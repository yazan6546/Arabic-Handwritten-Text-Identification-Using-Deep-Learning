{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sj_QqE4TfPX1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import torch.nn.functional as F\n",
    "from utilities import utils, process, evaluate, modify, plot\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data_path = '/content/drive/MyDrive/Arabic-Handwritten-Text-Identification-Using-Deep-Learning-main/Arabic-Handwritten-Text-Identification-Using-Deep-Learning-main/data'"
   ],
   "metadata": {
    "id": "6YJesf3Q38b4"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cp -r /content/drive/MyDrive/Arabic-Handwritten-Text-Identification-Using-Deep-Learning-main/Arabic-Handwritten-Text-Identification-Using-Deep-Learning-main/utilities ."
   ],
   "metadata": {
    "id": "CsVxGIlK8KI0"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = utils.load_images_to_dataframe(data_path + '/preprocessed')\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Target'] = label_encoder.fit_transform(df['Target'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# Further split the training set into training and validation sets\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "JVH4ne2v3eYv",
    "outputId": "da4ca3ad-82c4-4d6a-8942-005589db217c"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                          image  \\\n",
       "filename                                                                          \n",
       "user002_mustadhafeen_030.png  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "user071_qashtah_029.png       [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "user073_ghaleez_019.png       [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "user003_shateerah_031.png     [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "user004_sakhar_016.png        [[255, 255, 255, 255, 255, 255, 255, 255, 255,...   \n",
       "\n",
       "                              Target  \n",
       "filename                              \n",
       "user002_mustadhafeen_030.png       1  \n",
       "user071_qashtah_029.png           70  \n",
       "user073_ghaleez_019.png           72  \n",
       "user003_shateerah_031.png          2  \n",
       "user004_sakhar_016.png             3  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-14daa783-aeea-4d73-9921-798208b84b56\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user002_mustadhafeen_030.png</th>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user071_qashtah_029.png</th>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user073_ghaleez_019.png</th>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user003_shateerah_031.png</th>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user004_sakhar_016.png</th>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14daa783-aeea-4d73-9921-798208b84b56')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-14daa783-aeea-4d73-9921-798208b84b56 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-14daa783-aeea-4d73-9921-798208b84b56');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-3912c64b-75bd-465f-bab0-21ba87d4a7f7\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3912c64b-75bd-465f-bab0-21ba87d4a7f7')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-3912c64b-75bd-465f-bab0-21ba87d4a7f7 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8144,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8144,\n        \"samples\": [\n          \"user010_fasayakfeekahum_047.png\",\n          \"user046_mustadhafeen_026.png\",\n          \"user031_ghaleez_011.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23,\n        \"min\": 0,\n        \"max\": 81,\n        \"num_unique_values\": 82,\n        \"samples\": [\n          26,\n          1,\n          79\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Access the stored image data in grayscale\n",
    "        image = self.dataframe.iloc[idx, 0]  # 'image' column has the image data (grayscale)\n",
    "        label = self.dataframe.iloc[idx, 1]  # 'Target' column has the label\n",
    "\n",
    "        # Convert grayscale image to RGB if needed\n",
    "        image = np.expand_dims(image, axis=-1)  # Add channel dimension (H, W, 1)\n",
    "        image = np.repeat(image, 3, axis=-1)  # Convert to RGB by duplicating the grayscale channel\n",
    "\n",
    "        # Convert numpy array to PIL image\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "id": "oKacbW0Q3hKk"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data Augmentation**"
   ],
   "metadata": {
    "id": "SODaOiF6AJ04"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "transform_group1 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_group2 = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Group 2: Color Jitter\n",
    "transform_group3 = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Group 3: Resize\n",
    "transform_group4 = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "metadata": {
    "id": "iduU0cx5Cm9R"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset1 = ImageDataset(dataframe=train_df, transform=transform_group1)\n",
    "dataset2 = ImageDataset(dataframe=train_df, transform=transform_group2)\n",
    "dataset3 = ImageDataset(dataframe=train_df, transform=transform_group3)\n",
    "dataset4 = ImageDataset(dataframe=train_df, transform=transform_group4)"
   ],
   "metadata": {
    "id": "Gy924vc4Cc8f"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "combined_dataset = ConcatDataset([dataset1, dataset2, dataset3])"
   ],
   "metadata": {
    "id": "air6X5q4CuEW"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataloader = DataLoader(combined_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "id": "T8-xVVqqC8Ff"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(df['Target'].unique())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0glwbl_m3lT1",
    "outputId": "a0598285-a748-4363-c007-420bfa2f5738"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 1 70 72  2  3 73 31  4  9 16 49  0 11 27 37 14 80 76 39 19 69 41 79 62\n",
      " 25 48 52 36 44 35 26 60 45 81  7 40 24 57  8 55 68 78 63 58 23 64 33  5\n",
      " 50 51 66 15 29 32 12 53 30 74 38 21 56 42 17 28  6 59 65 71 75 47 18 46\n",
      " 10 34 77 13 67 61 43 54 22 20]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the transformation\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((128, 128)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # Create the dataset and dataloader\n",
    "# dataset = ImageDataset(dataframe=train_df, transform=transform)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "id": "fexdSvdKKscN"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the model class\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, stride, padding, fc_neurons, dropout_rate, pool_size, pool_type=\"max\"):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "        # Pooling layer selection (max or average pooling)\n",
    "        if pool_type == \"max\":\n",
    "            self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "        elif pool_type == \"avg\":\n",
    "            self.pool = nn.AvgPool2d(kernel_size=pool_size, stride=pool_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc1 = nn.Linear(num_filters * 2 * 16 * 16, fc_neurons)  # Initialize with a default size\n",
    "        self.fc2 = nn.Linear(fc_neurons, 82)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Dynamically determine the input size for fc1\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        if x.shape[1] != self.fc1.in_features:\n",
    "            self.fc1 = nn.Linear(x.shape[1], self.fc2.in_features)  # Adjust fc1 if needed\n",
    "            print(\"fc1 input size adjusted to:\", x.shape[1])\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _calculate_fc1_input_size(self, num_filters, kernel_size, stride, padding, pool_size):\n",
    "        # Create a dummy input tensor\n",
    "        dummy_input = torch.randn(1, 3, 128, 128)\n",
    "\n",
    "        # Pass the dummy input through the convolutional and pooling layers\n",
    "        dummy_output = self.pool(F.relu(nn.Conv2d(3, num_filters, kernel_size=kernel_size, stride=stride, padding=padding)(dummy_input)))\n",
    "        dummy_output = self.pool(F.relu(nn.Conv2d(num_filters, num_filters * 2, kernel_size=kernel_size, stride=stride, padding=padding)(dummy_output)))\n",
    "\n",
    "        # Calculate the flattened size of the output\n",
    "        return dummy_output.view(1, -1).shape[1]\n",
    "\n"
   ],
   "metadata": {
    "id": "5OBu354iKrJt"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Baseline configuration\n",
    "base_config = {\n",
    "    \"num_filters\": 16,\n",
    "    \"kernel_size\": 3,\n",
    "    \"stride\": 1,\n",
    "    \"padding\": 1,\n",
    "    \"fc_neurons\": 128,\n",
    "    \"dropout_rate\": 0,\n",
    "    \"pool_size\": 2,\n",
    "    \"pool_type\": \"max\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"optimizer_name\": \"Adam\",\n",
    "}\n",
    "\n",
    "# Parameters to tune (one at a time)\n",
    "parameters_to_tune = {\n",
    "    \"num_filters\": [16, 32],\n",
    "    \"kernel_size\": [3, 5],\n",
    "    \"stride\": [1,2],\n",
    "    \"padding\": [0,1],\n",
    "    \"fc_neurons\": [128,256],\n",
    "    \"dropout_rate\": [0,0.2],\n",
    "    \"pool_size\": [2,3],\n",
    "    \"pool_type\": [\"avg\"],\n",
    "    \"learning_rate\": [0.001,0.0005],\n",
    "    \"batch_size\": [32,64],\n",
    "    \"weight_decay\": [0.0,0.01],\n",
    "    \"optimizer_name\": [\"Adam\",\"SGD\"],\n",
    "}"
   ],
   "metadata": {
    "id": "TmyetfI_K1IP"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for param, values in parameters_to_tune.items():\n",
    "    for value in values:\n",
    "        # Update the current parameter in the base config\n",
    "        current_config = base_config.copy()\n",
    "        current_config[param] = value\n",
    "\n",
    "        # Create the dataloader with the updated batch size\n",
    "        dataloader = DataLoader(combined_dataset, batch_size=current_config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = SimpleCNN(\n",
    "            num_filters=current_config[\"num_filters\"],\n",
    "            kernel_size=current_config[\"kernel_size\"],\n",
    "            stride=current_config[\"stride\"],\n",
    "            padding=current_config[\"padding\"],\n",
    "            fc_neurons=current_config[\"fc_neurons\"],\n",
    "            dropout_rate=current_config[\"dropout_rate\"],\n",
    "            pool_size=current_config[\"pool_size\"],\n",
    "            pool_type=current_config[\"pool_type\"],\n",
    "        )\n",
    "\n",
    "        # Define optimizer\n",
    "        if current_config[\"optimizer_name\"] == \"Adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), lr=current_config[\"learning_rate\"], weight_decay=current_config[\"weight_decay\"])\n",
    "        elif current_config[\"optimizer_name\"] == \"SGD\":\n",
    "            optimizer = optim.SGD(model.parameters(), lr=current_config[\"learning_rate\"], weight_decay=current_config[\"weight_decay\"])\n",
    "\n",
    "        # Define loss function\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Training loop\n",
    "        num_epochs = 5\n",
    "        loss_per_epoch = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for images, labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                labels = labels.long()\n",
    "\n",
    "                # Forward pass (fc1 adjustment happens here if needed)\n",
    "                outputs = model(images)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_loss = running_loss / len(dataloader)\n",
    "            loss_per_epoch.append(avg_loss)\n",
    "            print(f'Tuning {param}={value}, Epoch={epoch+1}, Loss={avg_loss}')\n",
    "\n",
    "\n",
    "        # Save the model\n",
    "        model_name = f'model_{param}{value}.pth'\n",
    "        torch.save(model, os.path.join('/content/drive/MyDrive/cnn_models_augmented_data', model_name))\n",
    "\n",
    "        # Save the loss vs. epoch data\n",
    "        log_name = f'loss_{param}{value}.txt'\n",
    "        with open(os.path.join('/content/drive/MyDrive/logs_augmented', log_name), 'w') as f:\n",
    "            for epoch, loss in enumerate(loss_per_epoch, 1):\n",
    "                f.write(f'Epoch {epoch}, Loss: {loss}\\n')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "9BtuGn7wK4Fa",
    "outputId": "1e4779c5-6f66-4fda-b2d1-a0a4caf39dca"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fc1 input size adjusted to: 65536\n",
      "Tuning num_filters=16, Epoch=1, Loss=4.330350749275901\n",
      "Tuning num_filters=16, Epoch=2, Loss=3.8886219306425613\n",
      "Tuning num_filters=16, Epoch=3, Loss=3.716588181582364\n",
      "Tuning num_filters=16, Epoch=4, Loss=3.629050475467335\n",
      "Tuning num_filters=16, Epoch=5, Loss=3.5748000621795653\n",
      "fc1 input size adjusted to: 131072\n",
      "Tuning num_filters=32, Epoch=1, Loss=4.304433562972329\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-623ad077f9e7>\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m                 \u001B[0;31m# Forward pass (fc1 adjustment happens here if needed)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m                 \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m                 \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1735\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1736\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1737\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1746\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1748\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-16-6fc8f0d1551e>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0;31m# Dynamically determine the input size for fc1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1735\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1736\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1737\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1738\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1746\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1748\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    552\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    553\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 554\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    555\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    556\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    547\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroups\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    548\u001B[0m             )\n\u001B[0;32m--> 549\u001B[0;31m         return F.conv2d(\n\u001B[0m\u001B[1;32m    550\u001B[0m             \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdilation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroups\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    551\u001B[0m         )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Save the model's state_dict\n",
    "#torch.save(model.state_dict(), '/content/drive/MyDrive/cnn_proj_models/cnn_test')\n",
    "\n",
    "#To load it later:\n",
    "model = SimpleCNN(\n",
    "    num_filters=base_config[\"num_filters\"],\n",
    "    kernel_size=base_config[\"kernel_size\"],\n",
    "    stride=base_config[\"stride\"],\n",
    "    padding=base_config[\"padding\"],\n",
    "    fc_neurons=256,\n",
    "    dropout_rate=0.3,\n",
    "    pool_size=base_config[\"pool_size\"],\n",
    "    pool_type=base_config[\"pool_type\"]\n",
    ")\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/cnn_proj_models/model_fc_neurons256.pth'))\n"
   ],
   "metadata": {
    "id": "SHFvrRYEDLea",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4ebbedc5-ff7f-47da-dde4-22b9b0d20c2d"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-10-8483f8712ed5>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/content/drive/MyDrive/cnn_proj_models/model_fc_neurons256.pth'))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Assuming you already have your dataset and dataloaders ready\n",
    "# Example: train_loader, val_loader\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # or the appropriate loss for your task\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adjust learning rate if needed\n",
    "\n",
    "# Number of additional epochs to train\n",
    "num_epochs = 15  # Update this to your desired number\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous step\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rb7mRJUM8oqy",
    "outputId": "8153405c-092b-4c83-9c42-67517fa4487b"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15, Loss: 0.5153155087938776\n",
      "Epoch 2/15, Loss: 0.393408422631657\n",
      "Epoch 3/15, Loss: 0.3312929634247785\n",
      "Epoch 4/15, Loss: 0.2804775101173183\n",
      "Epoch 5/15, Loss: 0.2543026895829193\n",
      "Epoch 6/15, Loss: 0.2391717100981623\n",
      "Epoch 7/15, Loss: 0.2140477980517418\n",
      "Epoch 8/15, Loss: 0.20052805420456696\n",
      "Epoch 9/15, Loss: 0.1771318913720872\n",
      "Epoch 10/15, Loss: 0.1591620927312128\n",
      "Epoch 11/15, Loss: 0.15080908696045695\n",
      "Epoch 12/15, Loss: 0.13345368404913208\n",
      "Epoch 13/15, Loss: 0.12577146495206523\n",
      "Epoch 14/15, Loss: 0.11702878720572461\n",
      "Epoch 15/15, Loss: 0.11167127481641491\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), '/content/drive/MyDrive/cnn_proj_models/fc_neurons_256_epochs_15')"
   ],
   "metadata": {
    "id": "bPSi7yBFEvBV"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['Target'].value_counts()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "VC3_2rkE3q5o",
    "outputId": "b4c625f6-6f37-497b-afa8-7a19f34116e5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Target\n",
       "1     100\n",
       "68    100\n",
       "21    100\n",
       "38    100\n",
       "74    100\n",
       "     ... \n",
       "25    100\n",
       "62    100\n",
       "20    100\n",
       "47     94\n",
       "58     50\n",
       "Name: count, Length: 82, dtype: int64"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 1 columns</p>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "val_dataset = ImageDataset(dataframe=val_df, transform=transform)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# No need to track gradients during inference\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Iterate over the test dataset\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for images, labels in test_dataloader:\n",
    "        labels = labels.long()  # Ensure the labels are of type Long (int64)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Get the predicted class with the highest probability\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * correct_predictions / total_predictions\n",
    "print(f'Accuracy on the val set: {accuracy:.2f}%')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7IJpv-G3v92",
    "outputId": "73d8ad63-0fa1-43fe-a536-7e75d27de20d"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy on the val set: 30.98%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset = ImageDataset(dataframe=test_df, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# No need to track gradients during inference\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Iterate over the test dataset\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for images, labels in test_dataloader:\n",
    "        labels = labels.long()  # Ensure the labels are of type Long (int64)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Get the predicted class with the highest probability\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * correct_predictions / total_predictions\n",
    "print(f'Accuracy on the val set: {accuracy:.2f}%')"
   ],
   "metadata": {
    "id": "H7M37NFMEfZ8"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
