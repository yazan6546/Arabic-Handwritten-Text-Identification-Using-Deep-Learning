{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","name":"Arabic_deep"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10577711,"sourceType":"datasetVersion","datasetId":6545316}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\nimport kagglehub\nyazanaboeloun_arabic_path = kagglehub.dataset_download('yazanaboeloun/arabic')\n\nprint('Data source import complete.')\n","metadata":{"id":"c9gNak_feUKX","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:47:04.770380Z","iopub.execute_input":"2025-01-25T12:47:04.770652Z","iopub.status.idle":"2025-01-25T12:47:05.395051Z","shell.execute_reply.started":"2025-01-25T12:47:04.770632Z","shell.execute_reply":"2025-01-25T12:47:05.394381Z"}},"outputs":[{"name":"stdout","text":"Data source import complete.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\n\n# Define the dataset path\ndataset_path = yazanaboeloun_arabic_path\n\n# Check if the dataset path exists\nif os.path.exists(dataset_path):\n    print(\"Dataset path exists!\")\n    print(\"Files in the dataset directory:\")\n    print(os.listdir(dataset_path))  # List files to confirm the structure\nelse:\n    print(\"Dataset path does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:47:06.541098Z","iopub.execute_input":"2025-01-25T12:47:06.541424Z","iopub.status.idle":"2025-01-25T12:47:06.557415Z","shell.execute_reply.started":"2025-01-25T12:47:06.541399Z","shell.execute_reply":"2025-01-25T12:47:06.556641Z"}},"outputs":[{"name":"stdout","text":"Dataset path exists!\nFiles in the dataset directory:\n['.gitignore', 'preprocessed']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader,ConcatDataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"id":"sj_QqE4TfPX1","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:47:16.007924Z","iopub.execute_input":"2025-01-25T12:47:16.008223Z","iopub.status.idle":"2025-01-25T12:47:25.632586Z","shell.execute_reply.started":"2025-01-25T12:47:16.008193Z","shell.execute_reply":"2025-01-25T12:47:25.631945Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data_path = yazanaboeloun_arabic_path\ndataset_path = os.path.join(data_path, 'preprocessed')","metadata":{"id":"6YJesf3Q38b4","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:48:25.119536Z","iopub.execute_input":"2025-01-25T12:48:25.119816Z","iopub.status.idle":"2025-01-25T12:48:25.123478Z","shell.execute_reply.started":"2025-01-25T12:48:25.119795Z","shell.execute_reply":"2025-01-25T12:48:25.122761Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%cp -r /content/drive/MyDrive/Arabic-Handwritten-Text-Identification-Using-Deep-Learning-main/Arabic-Handwritten-Text-Identification-Using-Deep-Learning-main/utilities .","metadata":{"id":"CsVxGIlK8KI0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Images from File\ndef load_images_from_directory(directory, target_size=(256, 128)):\n    data = []\n    for root, _, files in os.walk(directory):\n\n        for filename in files:\n            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n                img = cv2.imread(os.path.join(root, filename), cv2.IMREAD_GRAYSCALE)\n                if img is not None:\n\n                    data.append({'filename': filename, 'image': img})\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:48:32.698357Z","iopub.execute_input":"2025-01-25T12:48:32.698642Z","iopub.status.idle":"2025-01-25T12:48:32.703552Z","shell.execute_reply.started":"2025-01-25T12:48:32.698622Z","shell.execute_reply":"2025-01-25T12:48:32.702730Z"},"id":"xAJpVxlzeUKb"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def load_images_to_dataframe(directory):\n    \"\"\"\n    Load images from a directory and return a DataFrame containing the images and their filenames.\n\n    Parameters:\n    directory (str): The directory containing the images.\n\n    Returns:\n    pd.DataFrame: A DataFrame with columns 'filename' and 'image'.\n    \"\"\"\n\n    data = load_images_from_directory(directory)\n    print('data')\n    df = pd.DataFrame(data)\n    df.set_index('filename', inplace=True)\n    df['Target'] = df.index.map(lambda x: x.split('_')[0])\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:48:35.000251Z","iopub.execute_input":"2025-01-25T12:48:35.000655Z","iopub.status.idle":"2025-01-25T12:48:35.006236Z","shell.execute_reply.started":"2025-01-25T12:48:35.000620Z","shell.execute_reply":"2025-01-25T12:48:35.005305Z"},"id":"heo4M0pAeUKc"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import cv2\nfrom matplotlib import pyplot as plt\n\n# Check the files and choose one to load\nimage_filename = os.listdir(dataset_path)[0]  # Replace with the actual image filename if needed\nimage_path = os.path.join(dataset_path, image_filename)\n\n# Load the image using OpenCV\nimage = cv2.imread(image_path)\n\n# Check if the image is loaded successfully\nif image is not None:\n    # Convert the image from BGR to RGB for correct display with Matplotlib\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Display the image\n    plt.imshow(image_rgb)\n    plt.axis(\"off\")  # Hide the axes\n    plt.title(f\"Loaded Image: {image_filename}\")\n    plt.show()\nelse:\n    print(f\"Failed to load image from {image_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:48:28.508768Z","iopub.execute_input":"2025-01-25T12:48:28.509040Z","iopub.status.idle":"2025-01-25T12:48:28.871626Z","shell.execute_reply.started":"2025-01-25T12:48:28.509020Z","shell.execute_reply":"2025-01-25T12:48:28.870798Z"},"id":"9AGkduXbeUKc","outputId":"1eb79c7e-7528-4c09-b2e7-50b73436d5d7"},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df = load_images_to_dataframe(dataset_path)\n\n# Encode the labels\nlabel_encoder = LabelEncoder()\ndf['Target'] = label_encoder.fit_transform(df['Target'])\n\n# Split the data into training and testing sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n# Further split the training set into training and validation sets\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"JVH4ne2v3eYv","outputId":"da4ca3ad-82c4-4d6a-8942-005589db217c","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:48:48.003655Z","iopub.execute_input":"2025-01-25T12:48:48.003945Z","iopub.status.idle":"2025-01-25T12:49:42.875364Z","shell.execute_reply.started":"2025-01-25T12:48:48.003923Z","shell.execute_reply":"2025-01-25T12:49:42.874525Z"}},"outputs":[{"name":"stdout","text":"data\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Custom Dataset class\nclass ImageDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        # Access the stored image data in grayscale\n        image = self.dataframe.iloc[idx, 0]  # 'image' column has the image data (grayscale)\n        label = self.dataframe.iloc[idx, 1]  # 'Target' column has the label\n\n        # Convert grayscale image to RGB if needed\n        image = np.expand_dims(image, axis=-1)  # Add channel dimension (H, W, 1)\n        image = np.repeat(image, 3, axis=-1)  # Convert to RGB by duplicating the grayscale channel\n\n        # Convert numpy array to PIL image\n        image = Image.fromarray(image)\n\n        # Apply transformations if any\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"id":"oKacbW0Q3hKk","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:50:16.917081Z","iopub.execute_input":"2025-01-25T12:50:16.917428Z","iopub.status.idle":"2025-01-25T12:50:16.923424Z","shell.execute_reply.started":"2025-01-25T12:50:16.917401Z","shell.execute_reply":"2025-01-25T12:50:16.922605Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**Data Augmentation**","metadata":{"id":"SODaOiF6AJ04"}},{"cell_type":"code","source":"transform_group1 = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor(),\n])\n\ntransform_group2 = transforms.Compose([\n    transforms.RandomRotation(degrees=15),\n    transforms.ToTensor(),\n])\n\n# Group 2: Color Jitter\ntransform_group3 = transforms.Compose([\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n])\n\n# Group 3: Resize\ntransform_group4 = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])","metadata":{"id":"iduU0cx5Cm9R"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset1 = ImageDataset(dataframe=train_df, transform=transform_group1)\ndataset2 = ImageDataset(dataframe=train_df, transform=transform_group2)\ndataset3 = ImageDataset(dataframe=train_df, transform=transform_group3)\ndataset4 = ImageDataset(dataframe=train_df, transform=transform_group4)","metadata":{"id":"Gy924vc4Cc8f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_dataset = ConcatDataset([dataset1, dataset2, dataset3])","metadata":{"id":"air6X5q4CuEW"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataloader = DataLoader(combined_dataset, batch_size=32, shuffle=True)","metadata":{"id":"T8-xVVqqC8Ff"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df['Target'].unique())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0glwbl_m3lT1","outputId":"a0598285-a748-4363-c007-420bfa2f5738"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 1 70 72  2  3 73 31  4  9 16 49  0 11 27 37 14 80 76 39 19 69 41 79 62\n"," 25 48 52 36 44 35 26 60 45 81  7 40 24 57  8 55 68 78 63 58 23 64 33  5\n"," 50 51 66 15 29 32 12 53 30 74 38 21 56 42 17 28  6 59 65 71 75 47 18 46\n"," 10 34 77 13 67 61 43 54 22 20]\n"]}],"execution_count":null},{"cell_type":"code","source":"# Define the transformation\n# transform = transforms.Compose([\n#     transforms.Resize((128, 128)),\n#     transforms.ToTensor(),\n# ])\n\n# # Create the dataset and dataloader\n# dataset = ImageDataset(dataframe=train_df, transform=transform)\n\n# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"id":"fexdSvdKKscN"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the model class\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_filters, kernel_size, stride, padding, fc_neurons, dropout_rate, pool_size, pool_type=\"max\"):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=kernel_size, stride=stride, padding=padding)\n\n        # Pooling layer selection (max or average pooling)\n        if pool_type == \"max\":\n            self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)\n        elif pool_type == \"avg\":\n            self.pool = nn.AvgPool2d(kernel_size=pool_size, stride=pool_size)\n\n        self.dropout = nn.Dropout(dropout_rate)\n\n        self.fc1 = nn.Linear(num_filters * 2 * 16 * 16, fc_neurons)  # Initialize with a default size\n        self.fc2 = nn.Linear(fc_neurons, 82)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n\n        # Dynamically determine the input size for fc1\n        x = x.view(x.size(0), -1)  # Flatten the output\n        if x.shape[1] != self.fc1.in_features:\n            self.fc1 = nn.Linear(x.shape[1], self.fc2.in_features)  # Adjust fc1 if needed\n            print(\"fc1 input size adjusted to:\", x.shape[1])\n\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n\n    def _calculate_fc1_input_size(self, num_filters, kernel_size, stride, padding, pool_size):\n        # Create a dummy input tensor\n        dummy_input = torch.randn(1, 3, 128, 128)\n\n        # Pass the dummy input through the convolutional and pooling layers\n        dummy_output = self.pool(F.relu(nn.Conv2d(3, num_filters, kernel_size=kernel_size, stride=stride, padding=padding)(dummy_input)))\n        dummy_output = self.pool(F.relu(nn.Conv2d(num_filters, num_filters * 2, kernel_size=kernel_size, stride=stride, padding=padding)(dummy_output)))\n\n        # Calculate the flattened size of the output\n        return dummy_output.view(1, -1).shape[1]\n\n","metadata":{"id":"5OBu354iKrJt"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Baseline configuration\nbase_config = {\n    \"num_filters\": 16,\n    \"kernel_size\": 3,\n    \"stride\": 1,\n    \"padding\": 1,\n    \"fc_neurons\": 128,\n    \"dropout_rate\": 0,\n    \"pool_size\": 2,\n    \"pool_type\": \"max\",\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n    \"weight_decay\": 0.0,\n    \"optimizer_name\": \"Adam\",\n}\n\n# Parameters to tune (one at a time)\nparameters_to_tune = {\n    \"num_filters\": [16, 32],\n    \"kernel_size\": [3, 5],\n    \"stride\": [1,2],\n    \"padding\": [0,1],\n    \"fc_neurons\": [128,256],\n    \"dropout_rate\": [0,0.2],\n    \"pool_size\": [2,3],\n    \"pool_type\": [\"avg\"],\n    \"learning_rate\": [0.001,0.0005],\n    \"batch_size\": [32,64],\n    \"weight_decay\": [0.0,0.01],\n    \"optimizer_name\": [\"Adam\",\"SGD\"],\n}","metadata":{"id":"TmyetfI_K1IP"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param, values in parameters_to_tune.items():\n    for value in values:\n        # Update the current parameter in the base config\n        current_config = base_config.copy()\n        current_config[param] = value\n\n        # Create the dataloader with the updated batch size\n        dataloader = DataLoader(combined_dataset, batch_size=current_config[\"batch_size\"], shuffle=True)\n\n        # Initialize the model\n        model = SimpleCNN(\n            num_filters=current_config[\"num_filters\"],\n            kernel_size=current_config[\"kernel_size\"],\n            stride=current_config[\"stride\"],\n            padding=current_config[\"padding\"],\n            fc_neurons=current_config[\"fc_neurons\"],\n            dropout_rate=current_config[\"dropout_rate\"],\n            pool_size=current_config[\"pool_size\"],\n            pool_type=current_config[\"pool_type\"],\n        )\n\n        # Define optimizer\n        if current_config[\"optimizer_name\"] == \"Adam\":\n            optimizer = optim.Adam(model.parameters(), lr=current_config[\"learning_rate\"], weight_decay=current_config[\"weight_decay\"])\n        elif current_config[\"optimizer_name\"] == \"SGD\":\n            optimizer = optim.SGD(model.parameters(), lr=current_config[\"learning_rate\"], weight_decay=current_config[\"weight_decay\"])\n\n        # Define loss function\n        criterion = nn.CrossEntropyLoss()\n\n        # Training loop\n        num_epochs = 5\n        loss_per_epoch = []\n\n        for epoch in range(num_epochs):\n            running_loss = 0.0\n            for images, labels in dataloader:\n                optimizer.zero_grad()\n                labels = labels.long()\n\n                # Forward pass (fc1 adjustment happens here if needed)\n                outputs = model(images)\n\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n\n            avg_loss = running_loss / len(dataloader)\n            loss_per_epoch.append(avg_loss)\n            print(f'Tuning {param}={value}, Epoch={epoch+1}, Loss={avg_loss}')\n\n\n        # Save the model\n        model_name = f'model_{param}{value}.pth'\n        torch.save(model, os.path.join('/content/drive/MyDrive/cnn_models_augmented_data', model_name))\n\n        # Save the loss vs. epoch data\n        log_name = f'loss_{param}{value}.txt'\n        with open(os.path.join('/content/drive/MyDrive/logs_augmented', log_name), 'w') as f:\n            for epoch, loss in enumerate(loss_per_epoch, 1):\n                f.write(f'Epoch {epoch}, Loss: {loss}\\n')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"9BtuGn7wK4Fa","outputId":"1e4779c5-6f66-4fda-b2d1-a0a4caf39dca"},"outputs":[{"output_type":"stream","name":"stdout","text":["fc1 input size adjusted to: 65536\n","Tuning num_filters=16, Epoch=1, Loss=4.330350749275901\n","Tuning num_filters=16, Epoch=2, Loss=3.8886219306425613\n","Tuning num_filters=16, Epoch=3, Loss=3.716588181582364\n","Tuning num_filters=16, Epoch=4, Loss=3.629050475467335\n","Tuning num_filters=16, Epoch=5, Loss=3.5748000621795653\n","fc1 input size adjusted to: 131072\n","Tuning num_filters=32, Epoch=1, Loss=4.304433562972329\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-623ad077f9e7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Forward pass (fc1 adjustment happens here if needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-6fc8f0d1551e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Dynamically determine the input size for fc1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":null},{"cell_type":"code","source":"#Save the model's state_dict\n#torch.save(model.state_dict(), '/content/drive/MyDrive/cnn_proj_models/cnn_test')\n\n#To load it later:\nmodel = SimpleCNN(\n    num_filters=base_config[\"num_filters\"],\n    kernel_size=base_config[\"kernel_size\"],\n    stride=base_config[\"stride\"],\n    padding=base_config[\"padding\"],\n    fc_neurons=256,\n    dropout_rate=0.3,\n    pool_size=base_config[\"pool_size\"],\n    pool_type=base_config[\"pool_type\"]\n)\nmodel.load_state_dict(torch.load('/content/drive/MyDrive/cnn_proj_models/model_fc_neurons256.pth'))\n","metadata":{"id":"SHFvrRYEDLea","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ebbedc5-ff7f-47da-dde4-22b9b0d20c2d"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-8483f8712ed5>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('/content/drive/MyDrive/cnn_proj_models/model_fc_neurons256.pth'))\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":10}],"execution_count":null},{"cell_type":"code","source":"# Assuming you already have your dataset and dataloaders ready\n# Example: train_loader, val_loader\n\n# Define your loss function and optimizer\ncriterion = nn.CrossEntropyLoss()  # or the appropriate loss for your task\noptimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adjust learning rate if needed\n\n# Number of additional epochs to train\nnum_epochs = 15  # Update this to your desired number\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    running_loss = 0.0\n\n    for inputs, labels in dataloader:\n        optimizer.zero_grad()  # Clear gradients from the previous step\n        outputs = model(inputs)  # Forward pass\n        loss = criterion(outputs, labels)  # Compute loss\n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update weights\n\n        running_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rb7mRJUM8oqy","outputId":"8153405c-092b-4c83-9c42-67517fa4487b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15, Loss: 0.5153155087938776\n","Epoch 2/15, Loss: 0.393408422631657\n","Epoch 3/15, Loss: 0.3312929634247785\n","Epoch 4/15, Loss: 0.2804775101173183\n","Epoch 5/15, Loss: 0.2543026895829193\n","Epoch 6/15, Loss: 0.2391717100981623\n","Epoch 7/15, Loss: 0.2140477980517418\n","Epoch 8/15, Loss: 0.20052805420456696\n","Epoch 9/15, Loss: 0.1771318913720872\n","Epoch 10/15, Loss: 0.1591620927312128\n","Epoch 11/15, Loss: 0.15080908696045695\n","Epoch 12/15, Loss: 0.13345368404913208\n","Epoch 13/15, Loss: 0.12577146495206523\n","Epoch 14/15, Loss: 0.11702878720572461\n","Epoch 15/15, Loss: 0.11167127481641491\n"]}],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), '/content/drive/MyDrive/cnn_proj_models/fc_neurons_256_epochs_15')","metadata":{"id":"bPSi7yBFEvBV"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Target'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"VC3_2rkE3q5o","outputId":"b4c625f6-6f37-497b-afa8-7a19f34116e5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Target\n","1     100\n","68    100\n","21    100\n","38    100\n","74    100\n","     ... \n","25    100\n","62    100\n","20    100\n","47     94\n","58     50\n","Name: count, Length: 82, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>Target</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>94</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>82 rows  1 columns</p>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":14}],"execution_count":null},{"cell_type":"code","source":"val_dataset = ImageDataset(dataframe=val_df, transform=transform)\ntest_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n\nmodel.eval()\n\n# No need to track gradients during inference\ncorrect_predictions = 0\ntotal_predictions = 0\n\n# Iterate over the test dataset\nwith torch.no_grad():  # Disable gradient calculations for evaluation\n    for images, labels in test_dataloader:\n        labels = labels.long()  # Ensure the labels are of type Long (int64)\n\n        # Forward pass\n        outputs = model(images)\n\n        # Get the predicted class with the highest probability\n        _, predicted = torch.max(outputs, 1)\n\n        # Count correct predictions\n        correct_predictions += (predicted == labels).sum().item()\n        total_predictions += labels.size(0)\n\n# Calculate the accuracy\naccuracy = 100 * correct_predictions / total_predictions\nprint(f'Accuracy on the val set: {accuracy:.2f}%')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7IJpv-G3v92","outputId":"73d8ad63-0fa1-43fe-a536-7e75d27de20d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on the val set: 30.98%\n"]}],"execution_count":null},{"cell_type":"code","source":"test_dataset = ImageDataset(dataframe=test_df, transform=transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n\nmodel.eval()\n\n# No need to track gradients during inference\ncorrect_predictions = 0\ntotal_predictions = 0\n\n# Iterate over the test dataset\nwith torch.no_grad():  # Disable gradient calculations for evaluation\n    for images, labels in test_dataloader:\n        labels = labels.long()  # Ensure the labels are of type Long (int64)\n\n        # Forward pass\n        outputs = model(images)\n\n        # Get the predicted class with the highest probability\n        _, predicted = torch.max(outputs, 1)\n\n        # Count correct predictions\n        correct_predictions += (predicted == labels).sum().item()\n        total_predictions += labels.size(0)\n\n# Calculate the accuracy\naccuracy = 100 * correct_predictions / total_predictions\nprint(f'Accuracy on the val set: {accuracy:.2f}%')","metadata":{"id":"H7M37NFMEfZ8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Task 3","metadata":{"id":"41WJAPOPeUKk"}},{"cell_type":"code","source":"# Define the transformation for ResNet and AlexNet\ntransform_resnet_alexnet = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:50:51.400928Z","iopub.execute_input":"2025-01-25T12:50:51.401285Z","iopub.status.idle":"2025-01-25T12:50:51.405524Z","shell.execute_reply.started":"2025-01-25T12:50:51.401255Z","shell.execute_reply":"2025-01-25T12:50:51.404850Z"},"id":"nVNuA1HueUKl"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"transform_group1 = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntransform_group2 = transforms.Compose([\n    transforms.RandomRotation(degrees=15),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Group 2: Color Jitter\ntransform_group3 = transforms.Compose([\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:50:52.585269Z","iopub.execute_input":"2025-01-25T12:50:52.585570Z","iopub.status.idle":"2025-01-25T12:50:52.590932Z","shell.execute_reply.started":"2025-01-25T12:50:52.585547Z","shell.execute_reply":"2025-01-25T12:50:52.590260Z"},"id":"jknj3beVeUKl"},"outputs":[],"execution_count":16},{"cell_type":"code","source":"dataset1 = ImageDataset(dataframe=train_df, transform=transform_group1)\ndataset2 = ImageDataset(dataframe=train_df, transform=transform_group2)\ndataset3 = ImageDataset(dataframe=train_df, transform=transform_group3)\ndataset4 = ImageDataset(dataframe=train_df, transform=transform_resnet_alexnet)\n\nval_dataset = ImageDataset(dataframe=val_df, transform=transform_resnet_alexnet)\ntest_dataset = ImageDataset(dataframe=test_df, transform=transform_resnet_alexnet)\n\ncombined_dataset = ConcatDataset([dataset1, dataset2, dataset3, dataset4])\n\ncombined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:50:55.183818Z","iopub.execute_input":"2025-01-25T12:50:55.184262Z","iopub.status.idle":"2025-01-25T12:50:55.191200Z","shell.execute_reply.started":"2025-01-25T12:50:55.184221Z","shell.execute_reply":"2025-01-25T12:50:55.190301Z"},"id":"gdSmQojeeUKl"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from torchvision import models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:50:57.339168Z","iopub.execute_input":"2025-01-25T12:50:57.339482Z","iopub.status.idle":"2025-01-25T12:50:57.342555Z","shell.execute_reply.started":"2025-01-25T12:50:57.339458Z","shell.execute_reply":"2025-01-25T12:50:57.341753Z"},"id":"QtvpuHODeUKl"},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Load a non-pretrained ResNet model\nmodel_resnet = models.resnet18(weights=None)\nnum_ftrs = model_resnet.fc.in_features\nmodel_resnet.fc = nn.Linear(num_ftrs, 82)  # Assuming 82 classes\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel_resnet = model_resnet.to(device)\n\n# Define the loss function and optimizer\ncriterion_resnet = nn.CrossEntropyLoss()\noptimizer_resnet = optim.Adam(model_resnet.parameters(), lr=0.00001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:51:01.850331Z","iopub.execute_input":"2025-01-25T12:51:01.850611Z","iopub.status.idle":"2025-01-25T12:51:02.451774Z","shell.execute_reply.started":"2025-01-25T12:51:01.850590Z","shell.execute_reply":"2025-01-25T12:51:02.451099Z"},"id":"z9TMz_0eeUKl"},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import json\n\n# Training loop with validation for ResNet\nnum_epochs = 50\nlog_data = {'epoch': [], 'training_loss': [], 'validation_loss': [], 'validation_accuracy': []}\n\nfor epoch in range(num_epochs):\n    model_resnet.train()\n    running_loss = 0.0\n    for images, labels in combined_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer_resnet.zero_grad()\n        outputs = model_resnet(images)\n        loss = criterion_resnet(outputs, labels)\n        loss.backward()\n        optimizer_resnet.step()\n        running_loss += loss.item()\n\n    training_loss = running_loss / len(combined_loader)\n    print(f'Epoch {epoch+1}, Training Loss: {training_loss}')\n\n    # Validation\n    model_resnet.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in val_dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model_resnet(images)\n            loss = criterion_resnet(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    validation_loss = val_loss / len(val_dataloader)\n    validation_accuracy = 100 * correct / total\n    print(f'Epoch {epoch+1}, Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}%')\n\n    # Save log data\n    log_data['epoch'].append(epoch + 1)\n    log_data['training_loss'].append(training_loss)\n    log_data['validation_loss'].append(validation_loss)\n    log_data['validation_accuracy'].append(validation_accuracy)\n\n# Save log data to a file\nwith open('training_log.json', 'w') as log_file:\n    json.dump(log_data, log_file)\n\nprint('Finished Training')\n\n# Save the model\ntorch.save(model_resnet.state_dict(), 'resnet_model_lr_0.001.pth')\n\n# # Evaluate the model on the test set\n# model_resnet.eval()\n# correct = 0\n# total = 0\n# with torch.no_grad():\n#     for images, labels in test_loader_resnet_alexnet:\n#         images, labels = images.to(device), labels.to(device)\n#         outputs = model_resnet(images)\n#         _, predicted = torch.max(outputs.data, 1)\n#         total += labels.size(0)\n#         correct += (predicted == labels).sum().item()\n\n# print(f'Accuracy on the test set: {100 * correct / total}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:37:58.263434Z","iopub.execute_input":"2025-01-25T11:37:58.263716Z"},"id":"BOvFBRZbeUKl","outputId":"1dedb6cb-6be1-4e0c-d87b-edb6f6215ba7"},"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 4.29192317312414\nEpoch 1, Validation Loss: 4.132739328202748, Validation Accuracy: 5.828220858895706%\nEpoch 2, Training Loss: 3.8401382255554197\nEpoch 2, Validation Loss: 3.726433072771345, Validation Accuracy: 11.042944785276074%\nEpoch 3, Training Loss: 3.4789125420830467\nEpoch 3, Validation Loss: 3.443730967385428, Validation Accuracy: 17.63803680981595%\nEpoch 4, Training Loss: 3.2151575977152045\nEpoch 4, Validation Loss: 3.2620488007863364, Validation Accuracy: 17.024539877300615%\nEpoch 5, Training Loss: 3.011609050577337\nEpoch 5, Validation Loss: 3.01163048971267, Validation Accuracy: 22.392638036809817%\nEpoch 6, Training Loss: 2.8272727649862115\nEpoch 6, Validation Loss: 2.8851920195988248, Validation Accuracy: 26.993865030674847%\nEpoch 7, Training Loss: 2.6860038705305618\nEpoch 7, Validation Loss: 2.7302442505246116, Validation Accuracy: 29.141104294478527%\nEpoch 8, Training Loss: 2.534316433993253\nEpoch 8, Validation Loss: 2.599452030091059, Validation Accuracy: 33.58895705521472%\nEpoch 9, Training Loss: 2.409257238561457\nEpoch 9, Validation Loss: 2.4840091977800642, Validation Accuracy: 33.895705521472394%\nEpoch 10, Training Loss: 2.2878652739524843\nEpoch 10, Validation Loss: 2.5079981031871976, Validation Accuracy: 32.36196319018405%\nEpoch 11, Training Loss: 2.161403056058017\nEpoch 11, Validation Loss: 2.3526501882643926, Validation Accuracy: 38.65030674846626%\nEpoch 12, Training Loss: 2.0391056472604925\nEpoch 12, Validation Loss: 2.3169357095445906, Validation Accuracy: 37.576687116564415%\nEpoch 13, Training Loss: 1.9362691773067822\nEpoch 13, Validation Loss: 2.1756497621536255, Validation Accuracy: 41.41104294478528%\nEpoch 14, Training Loss: 1.8344722884351556\nEpoch 14, Validation Loss: 2.127691075915382, Validation Accuracy: 42.02453987730061%\nEpoch 15, Training Loss: 1.729226859699596\nEpoch 15, Validation Loss: 2.0967013835906982, Validation Accuracy: 42.484662576687114%\nEpoch 16, Training Loss: 1.646663727977059\nEpoch 16, Validation Loss: 1.9845676933016096, Validation Accuracy: 45.70552147239264%\nEpoch 17, Training Loss: 1.5441300975192678\nEpoch 17, Validation Loss: 1.9962396508171445, Validation Accuracy: 46.012269938650306%\nEpoch 18, Training Loss: 1.463307610208338\nEpoch 18, Validation Loss: 1.9561160916373843, Validation Accuracy: 48.00613496932515%\nEpoch 19, Training Loss: 1.3908432479338213\nEpoch 19, Validation Loss: 1.8438323395592826, Validation Accuracy: 50.306748466257666%\nEpoch 20, Training Loss: 1.2895047761093485\nEpoch 20, Validation Loss: 1.800906675202506, Validation Accuracy: 50.61349693251534%\nEpoch 21, Training Loss: 1.2232081628929485\nEpoch 21, Validation Loss: 1.808845423516773, Validation Accuracy: 50.0%\nEpoch 22, Training Loss: 1.143150226094506\nEpoch 22, Validation Loss: 1.7289188986732846, Validation Accuracy: 51.68711656441718%\nEpoch 23, Training Loss: 1.084664592959664\nEpoch 23, Validation Loss: 1.737045617330642, Validation Accuracy: 52.607361963190186%\nEpoch 24, Training Loss: 1.0135972210493955\nEpoch 24, Validation Loss: 1.805275145031157, Validation Accuracy: 48.00613496932515%\nEpoch 25, Training Loss: 0.9490476621281017\nEpoch 25, Validation Loss: 1.6199593657539004, Validation Accuracy: 56.28834355828221%\nEpoch 26, Training Loss: 0.9036345227198167\nEpoch 26, Validation Loss: 1.6669971261705672, Validation Accuracy: 50.306748466257666%\nEpoch 27, Training Loss: 0.8441491264646703\nEpoch 27, Validation Loss: 1.585958963348752, Validation Accuracy: 53.22085889570552%\nEpoch 28, Training Loss: 0.8010774790156971\nEpoch 28, Validation Loss: 1.5241275514875139, Validation Accuracy: 54.4478527607362%\nEpoch 29, Training Loss: 0.7516927907141773\nEpoch 29, Validation Loss: 1.5610306206203641, Validation Accuracy: 57.515337423312886%\nEpoch 30, Training Loss: 0.7067045401984995\nEpoch 30, Validation Loss: 1.6655690556480771, Validation Accuracy: 52.147239263803684%\nEpoch 31, Training Loss: 0.6772893804311753\nEpoch 31, Validation Loss: 1.5369715917678106, Validation Accuracy: 56.74846625766871%\nEpoch 32, Training Loss: 0.6289244915138591\nEpoch 32, Validation Loss: 1.4805857681092762, Validation Accuracy: 58.895705521472394%\nEpoch 33, Training Loss: 0.6014004695415497\nEpoch 33, Validation Loss: 1.4218571413130987, Validation Accuracy: 56.74846625766871%\nEpoch 34, Training Loss: 0.5665476464683359\nEpoch 34, Validation Loss: 1.4342861459368752, Validation Accuracy: 59.20245398773006%\nEpoch 35, Training Loss: 0.5391302026943727\nEpoch 35, Validation Loss: 1.3975551696050734, Validation Accuracy: 59.355828220858896%\nEpoch 36, Training Loss: 0.5029191369902004\nEpoch 36, Validation Loss: 1.436492505527678, Validation Accuracy: 58.12883435582822%\nEpoch 37, Training Loss: 0.4777760656313463\nEpoch 37, Validation Loss: 1.3676427744683766, Validation Accuracy: 60.2760736196319%\nEpoch 38, Training Loss: 0.45579190877350895\nEpoch 38, Validation Loss: 1.4100542692911058, Validation Accuracy: 61.34969325153374%\nEpoch 39, Training Loss: 0.4288326754082333\nEpoch 39, Validation Loss: 1.4223148538952781, Validation Accuracy: 59.355828220858896%\nEpoch 40, Training Loss: 0.4032499183849855\nEpoch 40, Validation Loss: 1.380127830164773, Validation Accuracy: 61.19631901840491%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import json\n\ndef train_and_evaluate(model, criterion, optimizer, num_epochs, train_loader, val_loader, test_loader, device, model_name, lr):\n    log_data = {'epoch': [], 'training_loss': [], 'validation_loss': [], 'validation_accuracy': []}\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        training_loss = running_loss / len(train_loader)\n        print(f'Epoch {epoch+1}, Training Loss: {training_loss}')\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        validation_loss = val_loss / len(val_loader)\n        validation_accuracy = 100 * correct / total\n        print(f'Epoch {epoch+1}, Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}%')\n\n        # Save log data\n        log_data['epoch'].append(epoch + 1)\n        log_data['training_loss'].append(training_loss)\n        log_data['validation_loss'].append(validation_loss)\n        log_data['validation_accuracy'].append(validation_accuracy)\n\n    # Save log data to a file\n    log_filename = f'{model_name}_training_log_lr_{lr}_epochs_{num_epochs}.json'\n    with open(log_filename, 'w') as log_file:\n        json.dump(log_data, log_file)\n\n    print('Finished Training')\n\n    # Save the model\n    model_filename = f'{model_name}_model_lr_{lr}_epochs_{num_epochs}.pth'\n    torch.save(model.state_dict(), model_filename)\n\n    # Evaluate the model on the test set\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    test_accuracy = 100 * correct / total\n    print(f'Accuracy on the test set: {test_accuracy}%')\n    return test_accuracy","metadata":{"trusted":true,"id":"lilPTC8aeUKm","execution":{"iopub.status.busy":"2025-01-25T13:14:43.713628Z","iopub.execute_input":"2025-01-25T13:14:43.713955Z","iopub.status.idle":"2025-01-25T13:14:43.723065Z","shell.execute_reply.started":"2025-01-25T13:14:43.713928Z","shell.execute_reply":"2025-01-25T13:14:43.722389Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Hyperparameters\nlearning_rates = [0.001, 0.0001]\nnum_epochs_list = [10, 20]\n\n# Device configuration\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Loop over learning rates and number of epochs\nfor lr in learning_rates:\n    for num_epochs in num_epochs_list:\n        # ResNet\n        model_resnet = models.resnet18(pretrained=False)\n        num_ftrs = model_resnet.fc.in_features\n        model_resnet.fc = nn.Linear(num_ftrs, 82)  # Assuming 82 classes\n        model_resnet = model_resnet.to(device)\n        criterion_resnet = nn.CrossEntropyLoss()\n        optimizer_resnet = optim.Adam(model_resnet.parameters(), lr=lr)\n        print(f'Training ResNet with learning rate {lr} and {num_epochs} epochs')\n        train_and_evaluate(model_resnet, criterion_resnet, optimizer_resnet, num_epochs, combined_loader, val_dataloader, test_loader, device, 'resnet', lr)\n\n        # AlexNet\n        model_alexnet = models.alexnet(pretrained=False)\n        num_ftrs = model_alexnet.classifier[6].in_features\n        model_alexnet.classifier[6] = nn.Linear(num_ftrs, 82)  # Assuming 82 classes\n        model_alexnet = model_alexnet.to(device)\n        criterion_alexnet = nn.CrossEntropyLoss()\n        optimizer_alexnet = optim.Adam(model_alexnet.parameters(), lr=lr)\n        print(f'Training AlexNet with learning rate {lr} and {num_epochs} epochs')\n        train_and_evaluate(model_alexnet, criterion_alexnet, optimizer_alexnet, num_epochs, combined_loader, val_dataloader, test_loader, device, 'alexnet', lr)","metadata":{"trusted":true,"id":"wcmlS3NCeUKm","execution":{"iopub.status.busy":"2025-01-25T13:14:50.398411Z","iopub.execute_input":"2025-01-25T13:14:50.398704Z","iopub.status.idle":"2025-01-25T16:35:24.687890Z","shell.execute_reply.started":"2025-01-25T13:14:50.398679Z","shell.execute_reply":"2025-01-25T16:35:24.687099Z"}},"outputs":[{"name":"stdout","text":"Training ResNet with learning rate 0.001 and 10 epochs\nEpoch 1, Training Loss: 3.046637222841687\nEpoch 1, Validation Loss: 2.8783743495032903, Validation Accuracy: 25.613496932515336%\nEpoch 2, Training Loss: 1.9121149415696628\nEpoch 2, Validation Loss: 1.8058244841439384, Validation Accuracy: 46.93251533742331%\nEpoch 3, Training Loss: 1.1302042662853433\nEpoch 3, Validation Loss: 1.2340308058829534, Validation Accuracy: 62.576687116564415%\nEpoch 4, Training Loss: 0.6456110253218577\nEpoch 4, Validation Loss: 1.1971859960328965, Validation Accuracy: 66.71779141104294%\nEpoch 5, Training Loss: 0.40920266387082055\nEpoch 5, Validation Loss: 1.1773140941347395, Validation Accuracy: 67.94478527607362%\nEpoch 6, Training Loss: 0.2798094757429818\nEpoch 6, Validation Loss: 0.9391034131958371, Validation Accuracy: 73.77300613496932%\nEpoch 7, Training Loss: 0.2337063493951647\nEpoch 7, Validation Loss: 0.9814583786896297, Validation Accuracy: 74.69325153374233%\nEpoch 8, Training Loss: 0.18159484404989226\nEpoch 8, Validation Loss: 0.8152310216710681, Validation Accuracy: 76.99386503067484%\nEpoch 9, Training Loss: 0.16375196113163482\nEpoch 9, Validation Loss: 0.847483495871226, Validation Accuracy: 78.68098159509202%\nEpoch 10, Training Loss: 0.12619710123448508\nEpoch 10, Validation Loss: 0.9174950009300595, Validation Accuracy: 76.07361963190183%\nFinished Training\nAccuracy on the test set: 76.12031921424186%\nTraining AlexNet with learning rate 0.001 and 10 epochs\nEpoch 1, Training Loss: 4.416880121660427\nEpoch 1, Validation Loss: 4.412955193292527, Validation Accuracy: 1.0736196319018405%\nEpoch 2, Training Loss: 4.405319425288847\nEpoch 2, Validation Loss: 4.414044743492489, Validation Accuracy: 1.0736196319018405%\nEpoch 3, Training Loss: 4.404752074289777\nEpoch 3, Validation Loss: 4.415774981180827, Validation Accuracy: 1.0736196319018405%\nEpoch 4, Training Loss: 4.404417344362641\nEpoch 4, Validation Loss: 4.420106365567162, Validation Accuracy: 1.0736196319018405%\nEpoch 5, Training Loss: 4.404349983470339\nEpoch 5, Validation Loss: 4.419053054991222, Validation Accuracy: 1.0736196319018405%\nEpoch 6, Training Loss: 4.404210926565923\nEpoch 6, Validation Loss: 4.418541658492315, Validation Accuracy: 1.0736196319018405%\nEpoch 7, Training Loss: 4.404177267587201\nEpoch 7, Validation Loss: 4.420864264170329, Validation Accuracy: 1.0736196319018405%\nEpoch 9, Training Loss: 4.404121547246207\nEpoch 9, Validation Loss: 4.4210381507873535, Validation Accuracy: 1.0736196319018405%\nEpoch 10, Training Loss: 4.40411942652433\nEpoch 10, Validation Loss: 4.420405047280448, Validation Accuracy: 1.0736196319018405%\nFinished Training\nAccuracy on the test set: 0.6138735420503376%\nTraining ResNet with learning rate 0.001 and 20 epochs\nEpoch 1, Training Loss: 3.034906234942875\nEpoch 1, Validation Loss: 2.8558247884114585, Validation Accuracy: 26.380368098159508%\nEpoch 2, Training Loss: 1.9368339245999169\nEpoch 2, Validation Loss: 1.7145458516620455, Validation Accuracy: 51.99386503067485%\nEpoch 3, Training Loss: 1.1940836188412622\nEpoch 3, Validation Loss: 1.7807028463908605, Validation Accuracy: 48.92638036809816%\nEpoch 4, Training Loss: 0.6865365660328819\nEpoch 4, Validation Loss: 1.411519964536031, Validation Accuracy: 63.80368098159509%\nEpoch 5, Training Loss: 0.43843035484279164\nEpoch 5, Validation Loss: 1.280135330699739, Validation Accuracy: 66.41104294478528%\nEpoch 6, Training Loss: 0.30403520047441024\nEpoch 6, Validation Loss: 1.0875731025423323, Validation Accuracy: 71.0122699386503%\nEpoch 7, Training Loss: 0.22338920198177656\nEpoch 7, Validation Loss: 0.9411265921025049, Validation Accuracy: 74.23312883435582%\nEpoch 8, Training Loss: 0.19262338655976652\nEpoch 8, Validation Loss: 1.1467468795322238, Validation Accuracy: 71.62576687116564%\nEpoch 9, Training Loss: 0.16861422669960727\nEpoch 9, Validation Loss: 0.9184897286551339, Validation Accuracy: 74.38650306748467%\nEpoch 10, Training Loss: 0.14067353611992414\nEpoch 10, Validation Loss: 1.0142415251050676, Validation Accuracy: 75.920245398773%\nEpoch 11, Training Loss: 0.116015631255108\nEpoch 11, Validation Loss: 0.8744318740708488, Validation Accuracy: 77.76073619631902%\nEpoch 12, Training Loss: 0.11846442484756801\nEpoch 12, Validation Loss: 0.7951808146068028, Validation Accuracy: 81.59509202453988%\nEpoch 13, Training Loss: 0.09410324122052828\nEpoch 13, Validation Loss: 0.8152736581507183, Validation Accuracy: 79.9079754601227%\nEpoch 14, Training Loss: 0.0874386545125202\nEpoch 14, Validation Loss: 0.83565934357189, Validation Accuracy: 80.3680981595092%\nEpoch 15, Training Loss: 0.08298665332502468\nEpoch 15, Validation Loss: 0.9223610432375045, Validation Accuracy: 77.91411042944786%\nEpoch 16, Training Loss: 0.07096433660982998\nEpoch 16, Validation Loss: 0.9650640047731853, Validation Accuracy: 77.14723926380368%\nEpoch 17, Training Loss: 0.07519606527641141\nEpoch 17, Validation Loss: 0.799843559662501, Validation Accuracy: 80.21472392638037%\nEpoch 18, Training Loss: 0.06683043340404644\nEpoch 18, Validation Loss: 0.8719456579004016, Validation Accuracy: 79.9079754601227%\nEpoch 19, Training Loss: 0.05051111377384018\nEpoch 19, Validation Loss: 0.7536595790159135, Validation Accuracy: 82.66871165644172%\nEpoch 20, Training Loss: 0.06401061146227213\nEpoch 20, Validation Loss: 0.9410883174056098, Validation Accuracy: 78.83435582822086%\nFinished Training\nAccuracy on the test set: 80.23327194597913%\nTraining AlexNet with learning rate 0.001 and 20 epochs\nEpoch 1, Training Loss: 3.7637235750768228\nEpoch 1, Validation Loss: 3.1374587899162654, Validation Accuracy: 16.717791411042946%\nEpoch 2, Training Loss: 3.005984632575333\nEpoch 2, Validation Loss: 2.6935379845755443, Validation Accuracy: 26.533742331288344%\nEpoch 3, Training Loss: 2.6153793616314376\nEpoch 3, Validation Loss: 2.397419815971738, Validation Accuracy: 33.895705521472394%\nEpoch 4, Training Loss: 2.2972246291367453\nEpoch 4, Validation Loss: 2.2173997844968523, Validation Accuracy: 39.57055214723926%\nEpoch 5, Training Loss: 2.025642682227724\nEpoch 5, Validation Loss: 2.0850389968781244, Validation Accuracy: 40.33742331288344%\nEpoch 6, Training Loss: 1.8237313416775056\nEpoch 6, Validation Loss: 1.9908380281357538, Validation Accuracy: 48.15950920245399%\nEpoch 7, Training Loss: 1.6346783338757525\nEpoch 7, Validation Loss: 2.004253762108939, Validation Accuracy: 45.85889570552147%\nEpoch 8, Training Loss: 1.5090246916142525\nEpoch 8, Validation Loss: 1.9256504263196672, Validation Accuracy: 49.5398773006135%\nEpoch 9, Training Loss: 1.3678760404964099\nEpoch 9, Validation Loss: 1.9577896878832863, Validation Accuracy: 50.920245398773005%\nEpoch 10, Training Loss: 1.313863185792145\nEpoch 10, Validation Loss: 1.92292054494222, Validation Accuracy: 54.29447852760736%\nEpoch 11, Training Loss: 1.2062363252551949\nEpoch 11, Validation Loss: 1.8900250764120192, Validation Accuracy: 52.30061349693251%\nEpoch 12, Training Loss: 1.1667554711092054\nEpoch 12, Validation Loss: 1.9755420741580783, Validation Accuracy: 53.68098159509202%\nEpoch 13, Training Loss: 1.1201435920323715\nEpoch 13, Validation Loss: 2.0216639155433294, Validation Accuracy: 53.83435582822086%\nEpoch 14, Training Loss: 1.0679604385917216\nEpoch 14, Validation Loss: 1.9635334894770669, Validation Accuracy: 52.91411042944785%\nEpoch 15, Training Loss: 1.0581503843870852\nEpoch 15, Validation Loss: 2.099132674080985, Validation Accuracy: 52.147239263803684%\nEpoch 16, Training Loss: 1.0462391535061606\nEpoch 16, Validation Loss: 2.015892897333418, Validation Accuracy: 54.60122699386503%\nEpoch 17, Training Loss: 1.0451201597714002\nEpoch 17, Validation Loss: 1.9907375119981312, Validation Accuracy: 54.75460122699386%\nEpoch 18, Training Loss: 0.9885398232432699\nEpoch 18, Validation Loss: 2.1302626643862044, Validation Accuracy: 55.52147239263804%\nEpoch 19, Training Loss: 0.9446723648787846\nEpoch 19, Validation Loss: 2.002759030887059, Validation Accuracy: 57.20858895705521%\nEpoch 20, Training Loss: 0.9611300165786587\nEpoch 20, Validation Loss: 2.03801505338578, Validation Accuracy: 56.59509202453988%\nFinished Training\nAccuracy on the test set: 52.05647636586863%\nTraining ResNet with learning rate 0.0001 and 10 epochs\nEpoch 1, Training Loss: 3.1263952408123146\nEpoch 1, Validation Loss: 2.4400729622159685, Validation Accuracy: 33.282208588957054%\nEpoch 2, Training Loss: 1.875056836078144\nEpoch 2, Validation Loss: 1.9892690068199521, Validation Accuracy: 46.31901840490798%\nEpoch 3, Training Loss: 1.1405416178182886\nEpoch 3, Validation Loss: 1.6092989615031652, Validation Accuracy: 56.28834355828221%\nEpoch 4, Training Loss: 0.6979012898541733\nEpoch 4, Validation Loss: 1.561699872925168, Validation Accuracy: 54.29447852760736%\nEpoch 5, Training Loss: 0.488340405577039\nEpoch 5, Validation Loss: 1.1053846350737981, Validation Accuracy: 67.02453987730061%\nEpoch 6, Training Loss: 0.35362138082800587\nEpoch 6, Validation Loss: 1.1677614791052682, Validation Accuracy: 66.1042944785276%\nEpoch 7, Training Loss: 0.27510429532006686\nEpoch 7, Validation Loss: 1.004934461343856, Validation Accuracy: 69.6319018404908%\nEpoch 8, Training Loss: 0.21977696402600647\nEpoch 8, Validation Loss: 1.3197317236945743, Validation Accuracy: 61.50306748466258%\nEpoch 9, Training Loss: 0.18901213823651228\nEpoch 9, Validation Loss: 0.8881804872126806, Validation Accuracy: 72.85276073619632%\nEpoch 9, Validation Loss: 0.8881804872126806, Validation Accuracy: 72.85276073619632%\nEpoch 10, Training Loss: 0.16515823639166957\nEpoch 10, Validation Loss: 1.2944399430638267, Validation Accuracy: 62.11656441717791%\nFinished Training\nAccuracy on the test set: 62.98342541436464%\nTraining AlexNet with learning rate 0.0001 and 10 epochs\nEpoch 1, Training Loss: 3.6145425493362655\nEpoch 1, Validation Loss: 2.7252574421110607, Validation Accuracy: 24.233128834355828%\nEpoch 2, Training Loss: 2.4224214539196125\nEpoch 2, Validation Loss: 1.9795800844828289, Validation Accuracy: 43.40490797546012%\nEpoch 3, Training Loss: 1.583678055020992\nEpoch 3, Validation Loss: 1.692334566797529, Validation Accuracy: 52.760736196319016%\nEpoch 4, Training Loss: 0.9634686983705055\nEpoch 4, Validation Loss: 1.5117935424759275, Validation Accuracy: 60.88957055214724%\nEpoch 5, Training Loss: 0.6335039815953126\nEpoch 5, Validation Loss: 1.5701467309679304, Validation Accuracy: 62.576687116564415%\nEpoch 6, Training Loss: 0.46908966737505847\nEpoch 6, Validation Loss: 1.4354464809099834, Validation Accuracy: 66.41104294478528%\nEpoch 7, Training Loss: 0.37403324635707014\nEpoch 7, Validation Loss: 1.437342731725602, Validation Accuracy: 66.1042944785276%\nEpoch 8, Training Loss: 0.30147411009883773\nEpoch 8, Validation Loss: 1.583088681811378, Validation Accuracy: 66.71779141104294%\nEpoch 9, Training Loss: 0.27412996033783416\nEpoch 9, Validation Loss: 1.3034977373622714, Validation Accuracy: 69.78527607361963%\nEpoch 10, Training Loss: 0.23488623563881256\nEpoch 10, Validation Loss: 1.4396100824787503, Validation Accuracy: 65.95092024539878%\nFinished Training\nAccuracy on the test set: 66.42111724984653%\nTraining ResNet with learning rate 0.0001 and 20 epochs\nEpoch 1, Training Loss: 3.152253789758617\nEpoch 1, Validation Loss: 2.4657345839909146, Validation Accuracy: 31.441717791411044%\nEpoch 2, Training Loss: 1.940847852057682\nEpoch 2, Validation Loss: 1.78828303586869, Validation Accuracy: 51.99386503067485%\nEpoch 3, Training Loss: 1.2053478751306157\nEpoch 3, Validation Loss: 1.4201134954180037, Validation Accuracy: 58.74233128834356%\nEpoch 4, Training Loss: 0.7244494837107796\nEpoch 4, Validation Loss: 1.3203576973506383, Validation Accuracy: 59.20245398773006%\nEpoch 5, Training Loss: 0.47829137951908135\nEpoch 5, Validation Loss: 1.3644947778610956, Validation Accuracy: 61.65644171779141%\nEpoch 6, Training Loss: 0.3509939263128811\nEpoch 6, Validation Loss: 1.2337137687773931, Validation Accuracy: 64.87730061349693%\nEpoch 7, Training Loss: 0.2745881955587848\nEpoch 7, Validation Loss: 1.291579286257426, Validation Accuracy: 64.7239263803681%\nEpoch 8, Training Loss: 0.23063696702843148\nEpoch 8, Validation Loss: 1.0436831145059495, Validation Accuracy: 70.0920245398773%\nEpoch 9, Training Loss: 0.17740946204883096\nEpoch 9, Validation Loss: 0.8886713087558746, Validation Accuracy: 73.31288343558282%\nEpoch 10, Training Loss: 0.16357024144355872\nEpoch 10, Validation Loss: 1.305399097147442, Validation Accuracy: 65.49079754601227%\nEpoch 11, Training Loss: 0.13706212677569782\nEpoch 11, Validation Loss: 1.1480011358147575, Validation Accuracy: 69.32515337423312%\nEpoch 12, Training Loss: 0.11946570294900286\nEpoch 12, Validation Loss: 1.2307994280542647, Validation Accuracy: 65.6441717791411%\nEpoch 13, Training Loss: 0.10412207681629781\nEpoch 13, Validation Loss: 1.2024301631110055, Validation Accuracy: 68.40490797546012%\nEpoch 14, Training Loss: 0.09801398966674875\nEpoch 14, Validation Loss: 0.6860247240180061, Validation Accuracy: 78.83435582822086%\nEpoch 15, Training Loss: 0.09426626409207042\nEpoch 15, Validation Loss: 1.0247779431797208, Validation Accuracy: 72.85276073619632%\nEpoch 16, Training Loss: 0.07367789989660048\nEpoch 16, Validation Loss: 0.9430501872584933, Validation Accuracy: 74.079754601227%\nEpoch 17, Training Loss: 0.07792643397807628\nEpoch 17, Validation Loss: 0.818473322050912, Validation Accuracy: 76.99386503067484%\nEpoch 18, Training Loss: 0.06892060835294345\nEpoch 18, Validation Loss: 1.1230928301811218, Validation Accuracy: 69.93865030674847%\nEpoch 19, Training Loss: 0.059719197018806396\nEpoch 19, Validation Loss: 0.9826937175932384, Validation Accuracy: 72.69938650306749%\nEpoch 20, Training Loss: 0.06378483141761014\nEpoch 20, Validation Loss: 0.7166281597954887, Validation Accuracy: 80.67484662576688%\nFinished Training\nAccuracy on the test set: 79.31246163290362%\nTraining AlexNet with learning rate 0.0001 and 20 epochs\nEpoch 1, Training Loss: 3.6217984905997533\nEpoch 1, Validation Loss: 2.6580957004002164, Validation Accuracy: 25.766871165644172%\nEpoch 2, Training Loss: 2.3850204375398434\nEpoch 2, Validation Loss: 2.071299189612979, Validation Accuracy: 42.02453987730061%\nEpoch 3, Training Loss: 1.6004872335906244\nEpoch 3, Validation Loss: 1.7160676717758179, Validation Accuracy: 53.83435582822086%\nEpoch 4, Training Loss: 0.9961286210753583\nEpoch 4, Validation Loss: 1.7014783564068021, Validation Accuracy: 58.74233128834356%\nEpoch 5, Training Loss: 0.6378427820145515\nEpoch 5, Validation Loss: 1.5026153240885054, Validation Accuracy: 63.65030674846626%\nEpoch 6, Training Loss: 0.45324574019582015\nEpoch 6, Validation Loss: 1.4835129238310314, Validation Accuracy: 65.1840490797546%\nEpoch 7, Training Loss: 0.36190761875813526\nEpoch 7, Validation Loss: 1.4505624146688552, Validation Accuracy: 67.17791411042944%\nEpoch 8, Training Loss: 0.31683293876643476\nEpoch 8, Validation Loss: 1.584660998412541, Validation Accuracy: 65.6441717791411%\nEpoch 9, Training Loss: 0.26547622607733756\nEpoch 9, Validation Loss: 1.4022842127652395, Validation Accuracy: 69.1717791411043%\nEpoch 10, Training Loss: 0.23165127763710153\nEpoch 10, Validation Loss: 1.512096731435685, Validation Accuracy: 69.1717791411043%\nEpoch 11, Training Loss: 0.20684745327771564\nEpoch 11, Validation Loss: 1.477637909707569, Validation Accuracy: 69.1717791411043%\nEpoch 12, Training Loss: 0.180582795260375\nEpoch 12, Validation Loss: 1.392864125115531, Validation Accuracy: 71.93251533742331%\nEpoch 13, Training Loss: 0.17418932539949608\nEpoch 13, Validation Loss: 1.4721370197477794, Validation Accuracy: 71.47239263803681%\nEpoch 14, Training Loss: 0.14478848719528506\nEpoch 14, Validation Loss: 1.5938778633163089, Validation Accuracy: 70.5521472392638%\nEpoch 15, Training Loss: 0.14428839549272338\nEpoch 15, Validation Loss: 1.4873918550355094, Validation Accuracy: 70.5521472392638%\nEpoch 16, Training Loss: 0.13020543337927937\nEpoch 16, Validation Loss: 1.4949598056929452, Validation Accuracy: 71.31901840490798%\nEpoch 17, Training Loss: 0.12220019253627061\nEpoch 17, Validation Loss: 1.4416799204690116, Validation Accuracy: 71.16564417177914%\nEpoch 18, Training Loss: 0.1118168805071215\nEpoch 18, Validation Loss: 1.4934795967170171, Validation Accuracy: 71.47239263803681%\nEpoch 19, Training Loss: 0.10942209520809845\nEpoch 19, Validation Loss: 1.451487619252432, Validation Accuracy: 73.00613496932516%\nEpoch 20, Training Loss: 0.10503901902231734\nEpoch 20, Validation Loss: 1.5446279644966125, Validation Accuracy: 72.69938650306749%\nFinished Training\nAccuracy on the test set: 71.4548802946593%\n","output_type":"stream"}],"execution_count":25}]}